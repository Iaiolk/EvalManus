import asyncio
import json
import re
from typing import Any, Dict, List, Optional

from bs4 import BeautifulSoup
from openai import OpenAI
from pydantic import ConfigDict, Field, model_validator

from app.config import config
from app.logger import logger
from app.tool.base import BaseTool, ToolResult
from app.tool.search.base import SearchItem


class ModelSearchResult(SearchItem):
    """è¡¨ç¤ºæ¨¡å‹æœç´¢è¿”å›çš„å•ä¸ªæœç´¢ç»“æœã€‚"""

    source: str = Field(description="æœç´¢ç»“æœçš„æ¥æº")
    raw_content: Optional[str] = Field(
        default=None, description="æ¥è‡ªæœç´¢ç»“æœçš„åŸå§‹å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰"
    )


class SearchMetadata(ToolResult):
    """æœ‰å…³æœç´¢æ“ä½œçš„å…ƒæ•°æ®ã€‚"""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    total_results: int = Field(description="æ‰¾åˆ°çš„ç»“æœæ€»æ•°")
    language: str = Field(description="ç”¨äºæœç´¢çš„è¯­è¨€ä»£ç ")
    country: str = Field(description="ç”¨äºæœç´¢çš„å›½å®¶/åœ°åŒºä»£ç ")


class ModelSearchResponse(ToolResult):
    """æ¨¡å‹æœç´¢å·¥å…·çš„ç»“æ„åŒ–å“åº”ï¼Œç»§æ‰¿è‡ªToolResultã€‚"""

    query: str = Field(description="æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢")
    results: List[ModelSearchResult] = Field(
        default_factory=list, description="æœç´¢ç»“æœåˆ—è¡¨"
    )
    metadata: Optional[SearchMetadata] = Field(
        default=None, description="æœ‰å…³æœç´¢çš„å…ƒæ•°æ®"
    )

    @model_validator(mode="after")
    def populate_output(self) -> "ModelSearchResponse":
        """æ ¹æ®æœç´¢ç»“æœå¡«å……è¾“å‡ºæˆ–é”™è¯¯å­—æ®µã€‚"""
        if self.error:
            return self

        result_text = [f"'{self.query}'çš„æœç´¢ç»“æœï¼š"]

        for i, result in enumerate(self.results, 1):
            # æ·»åŠ å¸¦ä½ç½®ç¼–å·çš„æ ‡é¢˜
            title = result.title.strip() or "æ— æ ‡é¢˜"
            result_text.append(f"\n{i}. {title}")

            # æ·»åŠ å¸¦é€‚å½“ç¼©è¿›çš„URL
            result_text.append(f"   ç½‘å€: {result.url}")

            # å¦‚æœæœ‰æè¿°åˆ™æ·»åŠ 
            if result.description and result.description.strip():
                result_text.append(f"   æè¿°: {result.description}")

            # æ·»åŠ æ¥æºä¿¡æ¯
            result_text.append(f"   æ¥æº: {result.source}")

            # å¦‚æœæœ‰å†…å®¹é¢„è§ˆåˆ™æ·»åŠ 
            if result.raw_content:
                content_preview = result.raw_content[:1000].replace("\n", " ").strip()
                if len(result.raw_content) > 1000:
                    content_preview += "..."
                result_text.append(f"   å†…å®¹: {content_preview}")

        # å¦‚æœæœ‰å…ƒæ•°æ®åˆ™åœ¨åº•éƒ¨æ·»åŠ 
        if self.metadata:
            result_text.extend(
                [
                    f"\nå…ƒæ•°æ®:",
                    f"- æ€»ç»“æœæ•°: {self.metadata.total_results}",
                    f"- è¯­è¨€: {self.metadata.language}",
                    f"- å›½å®¶/åœ°åŒº: {self.metadata.country}",
                ]
            )

        self.output = "\n".join(result_text)
        return self


class ModelSearch(BaseTool):
    """ä½¿ç”¨deepseek-v3çš„LLMæœç´¢åŠŸèƒ½æœç´¢ç½‘ç»œï¼ˆé»˜è®¤é‡‡ç”¨ä¸­æ–‡æœç´¢æ¨¡å¼ï¼‰ã€‚"""

    name: str = "model_search"
    description: str = """ä½¿ç”¨deepseek-v3çš„å†…ç½®ç½‘ç»œæœç´¢åŠŸèƒ½æœç´¢å®æ—¶ä¿¡æ¯ï¼Œé»˜è®¤è¿”å›ä¸­æ–‡ç»“æœã€‚
    æ­¤å·¥å…·è¿”å›åŒ…å«ç›¸å…³ä¿¡æ¯ã€URLã€æ ‡é¢˜å’Œæè¿°çš„æœç´¢ç»“æœã€‚
    æœç´¢ç”±æ¨¡å‹çš„é›†æˆç½‘ç»œæœç´¢åŠŸèƒ½ç›´æ¥æ‰§è¡Œï¼Œé»˜è®¤é‡‡ç”¨ä¸­æ–‡æœç´¢å’Œå›ç­”æ¨¡å¼ã€‚"""
    parameters: dict = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "(å¿…å¡«) è¦æäº¤çš„æœç´¢æŸ¥è¯¢ã€‚",
            },
            "num_results": {
                "type": "integer",
                "description": "(å¯é€‰) è¦è¿”å›çš„æœ€å¤§æœç´¢ç»“æœæ•°ã€‚é»˜è®¤å€¼ä¸º5ã€‚",
                "default": 5,
            },
            "lang": {
                "type": "string",
                "description": "(å¯é€‰) æœç´¢ç»“æœçš„è¯­è¨€ä»£ç ï¼ˆé»˜è®¤å€¼ï¼šzhï¼Œè¡¨ç¤ºä¸­æ–‡ï¼‰ã€‚",
                "default": "zh",
            },
            "country": {
                "type": "string",
                "description": "(å¯é€‰) æœç´¢ç»“æœçš„å›½å®¶/åœ°åŒºä»£ç ï¼ˆé»˜è®¤å€¼ï¼šcnï¼Œè¡¨ç¤ºä¸­å›½ï¼‰ã€‚",
                "default": "cn",
            },
            "fetch_content": {
                "type": "boolean",
                "description": "(å¯é€‰) æ˜¯å¦è·å–æ›´è¯¦ç»†çš„å†…å®¹ã€‚é»˜è®¤å€¼ä¸ºfalseã€‚",
                "default": False,
            },
        },
        "required": ["query"],
    }

    model_config = ConfigDict(arbitrary_types_allowed=True)

    # åƒå¸†APIé…ç½®
    _base_url: str = "https://qianfan.baidubce.com/v2"
    _api_key: str = (
        "bce-v3/ALTAK-pOhfMH708hRvFf1pSDthj/141ada82dd49207be48d9bd013234550f9e04805"
    )
    _model: str = "deepseek-v3"
    client: Any = None

    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.client = OpenAI(
            base_url=self._base_url,
            api_key=self._api_key,
        )

    async def execute(
        self,
        query: str,
        num_results: int = 5,
        lang: Optional[str] = "zh",  # é»˜è®¤ä½¿ç”¨ä¸­æ–‡
        country: Optional[str] = "cn",  # é»˜è®¤ä½¿ç”¨ä¸­å›½åœ°åŒº
        fetch_content: bool = False,
    ) -> ModelSearchResponse:
        """
        ä½¿ç”¨deepseek-v3çš„å†…ç½®ç½‘ç»œæœç´¢åŠŸèƒ½æ‰§è¡Œæœç´¢ã€‚

        Args:
            query: è¦æäº¤çš„æœç´¢æŸ¥è¯¢
            num_results: è¦è¿”å›çš„æœ€å¤§æœç´¢ç»“æœæ•°ï¼ˆé»˜è®¤å€¼ï¼š5ï¼‰
            lang: æœç´¢ç»“æœçš„è¯­è¨€ä»£ç ï¼ˆé»˜è®¤å€¼ï¼š"zh"ï¼Œä¸­æ–‡ï¼‰
            country: æœç´¢ç»“æœçš„å›½å®¶/åœ°åŒºä»£ç ï¼ˆé»˜è®¤å€¼ï¼š"cn"ï¼Œä¸­å›½ï¼‰
            fetch_content: æ˜¯å¦è·å–æ›´è¯¦ç»†çš„å†…å®¹ï¼ˆé»˜è®¤å€¼ï¼šFalseï¼‰

        Returns:
            åŒ…å«æœç´¢ç»“æœå’Œå…ƒæ•°æ®çš„ç»“æ„åŒ–å“åº”
        """
        # å¦‚æœæä¾›äº†è¯­è¨€å‚æ•°ï¼Œåˆ™ä½¿ç”¨æä¾›çš„å€¼ï¼Œå¦åˆ™é»˜è®¤ä½¿ç”¨ä¸­æ–‡
        if lang is None:
            lang = (
                getattr(config.search_config, "lang", "zh")
                if config.search_config
                else "zh"
            )

        if country is None:
            country = (
                getattr(config.search_config, "country", "cn")
                if config.search_config
                else "cn"
            )

        # è·å–é‡è¯•é…ç½®
        max_retries = (
            getattr(config.search_config, "max_retries", 3)
            if config.search_config
            else 3
        )
        retry_delay = (
            getattr(config.search_config, "retry_delay", 5)
            if config.search_config
            else 5
        )

        # ç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼Œä¸æ„å»ºå¤æ‚çš„æç¤ºè¯
        # æ‰§è¡Œæœç´¢ï¼Œæœ€å¤šé‡è¯•æŒ‡å®šæ¬¡æ•°
        for attempt in range(max_retries):
            try:
                logger.info(
                    f"ğŸ” æ‰§è¡Œæ¨¡å‹æœç´¢ï¼š'{query}'ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰"
                )

                # è°ƒç”¨deepseek-v3çš„æœç´¢åŠŸèƒ½
                response_data = await self._perform_model_search(query)

                # è§£ææœç´¢ç»“æœ
                search_results = self._parse_search_results(
                    response_data, query, num_results, fetch_content
                )

                if search_results and len(search_results) > 0:
                    logger.info(f"âœ… æˆåŠŸè·å–åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ")

                    # è¿”å›æˆåŠŸçš„å“åº”
                    return ModelSearchResponse(
                        status="success",
                        query=query,
                        results=search_results,
                        metadata=SearchMetadata(
                            total_results=len(search_results),
                            language=lang,
                            country=country,
                        ),
                    )
            except Exception as e:
                logger.error(f"ğŸš¨ æ¨¡å‹æœç´¢å¤±è´¥: {str(e)}")
                if attempt < max_retries - 1:
                    logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)

        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥åè¿”å›é”™è¯¯å“åº”
        return ModelSearchResponse(
            query=query,
            error="æ— æ³•è·å–æœç´¢ç»“æœï¼Œæ‰€æœ‰å°è¯•å‡å¤±è´¥ã€‚",
            results=[],
        )

    async def _perform_model_search(self, query: str) -> dict:
        """
        ä½¿ç”¨æ·±åº¦æœç´¢æ¨¡å‹æ‰§è¡Œæœç´¢è¯·æ±‚ã€‚

        Args:
            query: æœç´¢æŸ¥è¯¢

        Returns:
            åŒ…å«æ¨¡å‹å“åº”å†…å®¹å’Œæœç´¢ç»“æœçš„å­—å…¸
        """

        # æ„å»ºæç¤ºè¯ï¼Œæ˜ç¡®è¦æ±‚ä¸­æ–‡ç»“æœå’Œå›ç­”
        enhanced_prompt = f"""{query}"""
        # ä½¿ç”¨å¼‚æ­¥è¿è¡ŒåŒæ­¥APIè°ƒç”¨
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self._model,
            messages=[{"content": enhanced_prompt, "role": "user"}],
            temperature=0.8,  # ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šæ€§çš„è¾“å‡º
            top_p=0.8,
            extra_body={
                "penalty_score": 1.5,
                "web_search": {"enable": True, "enable_trace": True},
                "response_language": "zh",  # æ˜ç¡®æŒ‡å®šè¿”å›ä¸­æ–‡ç»“æœ
            },
        )

        # è¿”å›å®Œæ•´å“åº”å¯¹è±¡ï¼ŒåŒ…å«å†…å®¹å’Œæœç´¢ç»“æœ
        result = {"content": "", "search_results": []}

        if response:
            # æå–å“åº”å†…å®¹
            if response.choices and len(response.choices) > 0:
                result["content"] = response.choices[0].message.content

            # æå–æœç´¢ç»“æœ
            result["search_results"] = self._extract_search_results_from_response(
                response
            )

        return result

    def _extract_search_results_from_response(self, response: Any) -> List[dict]:
        """
        ä»APIå“åº”ä¸­æå–æœç´¢ç»“æœã€‚

        Args:
            response: APIå“åº”å¯¹è±¡

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        search_results = []

        # å°è¯•ä»response.search_resultsè·å–
        if hasattr(response, "search_results") and response.search_results:
            search_results = response.search_results
            logger.info(
                f"ä»response.search_resultsæå–åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ"
            )

        # å¦‚æœresponseæœ¬èº«å°±æ˜¯ä¸€ä¸ªå­—å…¸å¹¶ä¸”åŒ…å«search_results
        elif isinstance(response, dict) and "search_results" in response:
            search_results = response["search_results"]
            logger.info(
                f"ä»response[search_results]æå–åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ"
            )

        # å°è¯•ä»response.search_infoæˆ–å…¶ä»–å±æ€§ä¸­æå–
        elif hasattr(response, "search_info") and hasattr(
            response.search_info, "results"
        ):
            search_results = response.search_info.results
            logger.info(
                f"ä»response.search_info.resultsæå–åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ"
            )

        # å°è¯•ä»å“åº”å†…å®¹ä¸­æå–æœç´¢å¼•ç”¨URL
        elif hasattr(response, "choices") and len(response.choices) > 0:
            content = response.choices[0].message.content
            # æå–å¯èƒ½åŒ…å«çš„URLå¼•ç”¨
            urls = self._extract_urls_from_text(content)
            if urls:
                logger.info(f"ä»å“åº”å†…å®¹ä¸­æå–åˆ° {len(urls)} ä¸ªURL")
                for i, url in enumerate(urls):
                    search_results.append(
                        {"title": f"æœç´¢ç»“æœ {i+1}", "url": url, "index": i + 1}
                    )

        # è®°å½•æå–ç»“æœ
        if search_results:
            logger.info(f"æ€»å…±ä»APIå“åº”ä¸­æå–åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ")
        else:
            logger.warning("æœªèƒ½ä»APIå“åº”ä¸­æå–åˆ°ä»»ä½•æœç´¢ç»“æœ")

        return search_results

    def _extract_urls_from_text(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–URLã€‚

        Args:
            text: å¯èƒ½åŒ…å«URLçš„æ–‡æœ¬

        Returns:
            æå–çš„URLåˆ—è¡¨
        """
        if not text:
            return []

        import re

        # URLæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œæ”¯æŒä¸­æ–‡åŸŸå
        url_pattern = r'https?://[^\s<>"\'()]+|www\.[^\s<>"\'()]+'

        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…
        urls = re.findall(url_pattern, text)

        # å¯¹äºä¸ä»¥httpå¼€å¤´çš„URLï¼Œæ·»åŠ å‰ç¼€
        processed_urls = []
        for url in urls:
            if not url.startswith(("http://", "https://")):
                url = "http://" + url
            processed_urls.append(url)

        return processed_urls

    def _parse_search_results(
        self, response_data: dict, query: str, num_results: int, fetch_content: bool
    ) -> List[ModelSearchResult]:
        """
        è§£ææ¨¡å‹è¿”å›çš„æœç´¢ç»“æœã€‚

        Args:
            response_data: æ¨¡å‹è¿”å›çš„æ•°æ®ï¼ŒåŒ…å«å†…å®¹å’Œæœç´¢ç»“æœ
            query: åŸå§‹æŸ¥è¯¢
            num_results: æœŸæœ›çš„ç»“æœæ•°é‡
            fetch_content: æ˜¯å¦è·å–æ›´è¯¦ç»†å†…å®¹

        Returns:
            è§£æåçš„æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        content = response_data.get("content", "")
        search_results = response_data.get("search_results", [])

        # é¦–å…ˆå°è¯•ç›´æ¥è§£æAPIè¿”å›çš„æœç´¢ç»“æœ
        if search_results and isinstance(search_results, list):
            logger.info(f"ä»APIæœç´¢ç»“æœä¸­æå–åˆ° {len(search_results)} ä¸ªæ¡ç›®")

            # é™åˆ¶ç»“æœæ•°é‡
            search_results = (
                search_results[:num_results]
                if len(search_results) > num_results
                else search_results
            )

            for i, item in enumerate(search_results):
                if not isinstance(item, dict):
                    continue

                try:
                    # æå–æ ‡é¢˜ã€URLå’Œç´¢å¼•
                    title = item.get("title", "")
                    url = item.get("url", "")
                    index = item.get("index", i + 1)

                    # ä¼˜åŒ–URLå¤„ç†ï¼Œç¡®ä¿ä¸­æ–‡URLæ­£ç¡®å¤„ç†
                    if url and not (
                        url.startswith("http://") or url.startswith("https://")
                    ):
                        url = "http://" + url

                    # å¦‚æœURLéç©ºä½†titleä¸ºç©ºï¼Œä»URLç”Ÿæˆä¸€ä¸ªæ ‡é¢˜
                    if url and not title:
                        # ä»URLä¸­æå–åŸŸåä½œä¸ºæ ‡é¢˜
                        domain_match = re.search(r"https?://(?:www\.)?([^/]+)", url)
                        if domain_match:
                            title = f"{domain_match.group(1)} çš„æœç´¢ç»“æœ"
                        else:
                            title = f"ç»“æœ {index}"

                    # ä½¿ç”¨æœç´¢ç»“æœçš„snippetæˆ–contentä½œä¸ºæè¿°
                    description = item.get("snippet", "") or item.get("content", "")

                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æè¿°ï¼Œä½¿ç”¨å®Œæ•´çš„æ¨¡å‹å›ç­”ä½œä¸ºæè¿°
                    if not description:
                        description = (
                            content[:300] + "..." if len(content) > 300 else content
                        )

                    results.append(
                        ModelSearchResult(
                            title=title or f"ç»“æœ {index}",
                            url=url or "",
                            description=description,
                            source="deepseek-v3",
                            raw_content=content if fetch_content else None,
                        )
                    )
                except Exception as e:
                    logger.error(f"è§£æAPIæœç´¢ç»“æœé¡¹æ—¶å‡ºé”™: {str(e)}")

            if results:
                return results

        # å¦‚æœæ— æ³•ä»APIè·å–æœç´¢ç»“æœï¼Œå›é€€åˆ°ä»å†…å®¹ä¸­æå–JSON
        try:
            # å°è¯•ä»å“åº”å†…å®¹ä¸­æå–JSON
            json_match = self._extract_json_from_text(content)

            if json_match:
                # è§£æJSONç»“æœ
                json_data = json.loads(json_match)

                # å¤„ç†å¯èƒ½çš„ä¸åŒJSONç»“æ„
                items = []
                if isinstance(json_data, list):
                    items = json_data
                elif isinstance(json_data, dict) and "results" in json_data:
                    items = json_data["results"]
                elif isinstance(json_data, dict) and "items" in json_data:
                    items = json_data["items"]

                # é™åˆ¶ç»“æœæ•°é‡
                items = items[:num_results] if items else []

                # åˆ›å»ºModelSearchResultå¯¹è±¡
                for i, item in enumerate(items):
                    if not isinstance(item, dict):
                        continue

                    try:
                        results.append(
                            ModelSearchResult(
                                title=item.get("title", f"Result {i+1}"),
                                url=item.get("url", ""),
                                description=item.get("description", ""),
                                source="deepseek-v3",
                                raw_content=(
                                    item.get("content", "") if fetch_content else None
                                ),
                            )
                        )
                    except Exception as e:
                        logger.error(f"è§£ææœç´¢ç»“æœé¡¹æ—¶å‡ºé”™: {str(e)}")
        except Exception as e:
            logger.error(f"è§£ææœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")

        # å¦‚æœæ²¡æœ‰æå–åˆ°ç»“æœï¼Œå°è¯•ä»æ–‡æœ¬ä¸­åˆ›å»ºç®€å•ç»“æœ
        if not results:
            logger.warning("æœªèƒ½è§£æç»“æœï¼Œå°è¯•ä»æ–‡æœ¬åˆ›å»ºç®€å•ç»“æœ")
            results = self._create_simple_result_from_text(content, query)

        return results

    def _extract_json_from_text(self, text: str) -> Optional[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–JSONå­—ç¬¦ä¸²ã€‚

        Args:
            text: å¯èƒ½åŒ…å«JSONçš„æ–‡æœ¬

        Returns:
            æå–çš„JSONå­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        # å°è¯•æ‰¾åˆ°JSONä»£ç å—
        import re

        json_pattern = r"```(?:json)?\n([\s\S]*?)\n```"
        json_matches = re.findall(json_pattern, text)

        if json_matches:
            return json_matches[0]

        # å°è¯•æ‰¾åˆ°å¤§æ‹¬å·åŒ…å›´çš„æ•´ä¸ªæ–‡æœ¬
        if text.strip().startswith("{") and text.strip().endswith("}"):
            return text

        # å°è¯•æ‰¾åˆ°æ–¹æ‹¬å·åŒ…å›´çš„æ•´ä¸ªæ–‡æœ¬
        if text.strip().startswith("[") and text.strip().endswith("]"):
            return text

        return None

    def _create_simple_result_from_text(
        self, text: str, query: str
    ) -> List[ModelSearchResult]:
        """
        å½“JSONè§£æå¤±è´¥æ—¶ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­åˆ›å»ºç®€å•æœç´¢ç»“æœã€‚

        Args:
            text: æ¨¡å‹è¿”å›çš„æ–‡æœ¬
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            åˆ›å»ºçš„æœç´¢ç»“æœåˆ—è¡¨
        """
        # åˆ›å»ºå•ä¸ªæœç´¢ç»“æœï¼Œç¡®ä¿æ ‡é¢˜å’Œæè¿°éƒ½ä½¿ç”¨ä¸­æ–‡
        return [
            ModelSearchResult(
                title=f"é—®é¢˜{query}çš„æœç´¢ç»“æœ",
                url="",
                description=text[:500] + ("..." if len(text) > 500 else ""),
                source="deepseek-v3ï¼ˆä¸­æ–‡æœç´¢ï¼‰",
                raw_content=text,
            )
        ]


if __name__ == "__main__":

    async def test_search():
        search_tool = ModelSearch()
        response = await search_tool.execute(
            query="æŠ—æ—¥æ—¶å±±ä¸œä¸‰æ”¯é˜Ÿé•¿æ˜¯è°æ¨å›½å¤«åœ¨å±±ä¸œå…ˆåä»€ä¹ˆèŒåŠ¡",
            fetch_content=True,
            num_results=3,
            # ä¸éœ€è¦æ˜¾å¼æŒ‡å®šlangå’Œcountryå‚æ•°ï¼Œå› ä¸ºé»˜è®¤å·²ç»æ˜¯"zh"å’Œ"cn"
        )
        print(response.output)

    asyncio.run(test_search())
